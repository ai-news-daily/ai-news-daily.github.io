name: Daily AI News Crawl with LLM

on:
  schedule:
    - cron: '0 6,18 * * *'  # Runs at 6 AM and 6 PM UTC daily
  workflow_dispatch:  # Manual trigger for testing
  push:
    branches: [main]  # Also run on pushes to main for testing

jobs:
  crawl-and-build:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Allow time for model downloads
    
    permissions:
      contents: write  # Allow pushing changes
      pages: write     # Allow GitHub Pages deployment
      id-token: write  # Required for pages deployment
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Cache Transformers models
        uses: actions/cache@v3
        with:
          path: |
            .cache
            ~/.cache/huggingface
          key: ${{ runner.os }}-transformers-models-v2
          restore-keys: |
            ${{ runner.os }}-transformers-models-
            
      - name: Install dependencies
        run: |
          npm ci
          echo "Dependencies installed"
          
      - name: Pre-download AI models (if not cached)
        run: |
          echo "Pre-downloading AI models..."
          timeout 300 node -e "
            import('@xenova/transformers').then(async ({pipeline, env}) => {
              env.cacheDir = './.cache';
              console.log('Loading classifier...');
              await pipeline('zero-shot-classification', 'Xenova/bart-large-mnli');
              console.log('Loading language detector...');
              await pipeline('text-classification', 'Xenova/xlm-roberta-base-language-detection');
              console.log('Models ready!');
            }).catch(console.error);
          " || echo "Model download timeout - will download during processing"
          
      - name: Create directories
        run: |
          mkdir -p data
          mkdir -p site
          mkdir -p .cache
          
      - name: Crawl RSS feeds
        run: |
          echo "ðŸ¤– Starting RSS crawl..."
          node scripts/crawl.js
          echo "Crawl completed"
          
      - name: Process with AI categorization
        run: |
          echo "ðŸ§  Processing articles with AI..."
          node scripts/process-with-llm.js
          echo "AI processing completed"
          
      - name: Build static site
        run: |
          echo "ðŸ—ï¸ Building static site..."
          node scripts/build-site.js
          echo "Site build completed"
          
      - name: Generate sitemap
        run: |
          echo "ðŸ—ºï¸ Generating sitemap..."
          cat > site/sitemap.xml << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
            <url>
              <loc>https://ai-news-daily.github.io/</loc>
              <changefreq>daily</changefreq>
              <priority>1.0</priority>
              <lastmod>$(date -u +%Y-%m-%dT%H:%M:%S+00:00)</lastmod>
            </url>
          </urlset>
          EOF
          
      - name: Generate robots.txt
        run: |
          cat > site/robots.txt << 'EOF'
          User-agent: *
          Allow: /
          
          Sitemap: https://ai-news-daily.github.io/sitemap.xml
          EOF
          
      - name: Optimize and validate
        run: |
          # Validate JSON files
          echo "ðŸ” Validating data files..."
          node -e "
            const fs = require('fs');
            try {
              JSON.parse(fs.readFileSync('data/latest.json'));
              console.log('âœ… Data files valid');
            } catch(e) {
              console.error('âŒ Invalid JSON:', e.message);
              process.exit(1);
            }
          "
          
          # Check if we have articles
          ARTICLE_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/latest.json')).articles.length)")
          echo "ðŸ“Š Found $ARTICLE_COUNT articles"
          
          if [ "$ARTICLE_COUNT" -lt 10 ]; then
            echo "âš ï¸ Warning: Low article count ($ARTICLE_COUNT)"
          fi
          
      - name: Commit and push changes
        run: |
          git config --local user.email "ai-news-bot@users.noreply.github.com"
          git config --local user.name "AI News Bot"
          git add data/ site/
          
          # Create commit message with stats
          ARTICLE_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/latest.json')).articles.length)")
          TIMESTAMP=$(date -u '+%Y-%m-%d %H:%M UTC')
          
          git commit -m "ðŸ¤– Update AI news - $ARTICLE_COUNT articles - $TIMESTAMP" || {
            echo "No changes to commit"
            exit 0
          }
          
          git push
          
      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v2
        with:
          path: ./site
          
      - name: Report status
        run: |
          echo "âœ… Workflow completed successfully!"
          echo "ðŸ“Š Statistics:"
          node -e "
            const data = JSON.parse(require('fs').readFileSync('data/latest.json'));
            console.log(\`Articles: \${data.articles.length}\`);
            console.log(\`Categories: \${data.categories.length}\`);
            console.log(\`Duplicates: \${data.duplicateGroups}\`);
          "
          
      - name: Cleanup old data files
        run: |
          # Keep only last 30 days of data files
          find data/ -name "*.json" -type f -mtime +30 -delete || true
          echo "ðŸ§¹ Cleaned up old data files"
          
  # Health check job
  health-check:
    needs: crawl-and-build
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Check deployment health
        run: |
          sleep 60  # Wait for deployment
          
          # Check if site is accessible
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" "https://ai-news-daily.github.io/" || echo "000")
          
          if [ "$STATUS" = "200" ]; then
            echo "âœ… Site is healthy (HTTP $STATUS)"
          else
            echo "âŒ Site health check failed (HTTP $STATUS)"
            exit 1
          fi
          
      - name: Notify on failure
        if: failure()
        run: |
          echo "âŒ Workflow failed! Check the logs."
          # Here you could add notifications (email, Slack, etc.)
          
# Environment variables for the workflow
env:
  NODE_OPTIONS: '--max-old-space-size=4096'  # Increase memory for AI models
  FORCE_COLOR: 1  # Enable colors in terminal output 