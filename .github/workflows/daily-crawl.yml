name: Daily AI News Crawl with LLM

on:
  schedule:
    - cron: '0 6,18 * * *'  # Runs at 6 AM and 6 PM UTC daily
  workflow_dispatch:  # Manual trigger for testing
  push:
    branches: [main]  # Also run on pushes to main for testing

jobs:
  crawl-and-build:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Allow time for model downloads
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    permissions:
      contents: write  # Allow pushing changes
      pages: write     # Allow GitHub Pages deployment
      id-token: write  # Required for pages deployment
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Cache Transformers models
        uses: actions/cache@v3
        with:
          path: |
            .cache
            ~/.cache/huggingface
          key: ${{ runner.os }}-transformers-models-v2
          restore-keys: |
            ${{ runner.os }}-transformers-models-
            
      - name: Install dependencies
        run: |
          npm ci
          echo "Dependencies installed"
          
      - name: Pre-download AI models (if not cached)
        run: |
          echo "Pre-downloading AI models..."
          timeout 300 node -e "
            import('@xenova/transformers').then(async ({pipeline, env}) => {
              env.cacheDir = './.cache';
              console.log('Loading classifier...');
              await pipeline('zero-shot-classification', 'Xenova/distilbert-base-uncased-mnli');
              console.log('Loading summarizer...');
              await pipeline('summarization', 'Xenova/distilbart-cnn-6-6');
              console.log('Loading NER...');
              await pipeline('ner', 'Xenova/bert-base-NER');
              console.log('Models ready!');
            }).catch(console.error);
          " || echo "Model download timeout - will download during processing"
          
      - name: Create directories
        run: |
          mkdir -p data
          mkdir -p site
          mkdir -p .cache
          
      - name: Crawl RSS feeds
        run: |
          echo "ü§ñ Starting RSS crawl..."
          node scripts/crawl.js
          echo "Crawl completed"
          
      - name: Process with AI categorization
        # env:
          # PROCESSING_LIMIT: 20  # Process only 20 articles for faster builds
        run: |
          echo "üß† Processing articles with AI (no limits - full processing)..."
          node scripts/process-clean.js
          echo "AI processing completed"
          
      - name: Build static site
        run: |
          echo "üèóÔ∏è Building static site..."
          node scripts/build-site.js
          echo "Site build completed"
          
      - name: Generate sitemap
        run: |
          echo "üó∫Ô∏è Generating sitemap..."
          cat > site/sitemap.xml << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
            <url>
              <loc>https://ai-news-daily.github.io/</loc>
              <changefreq>daily</changefreq>
              <priority>1.0</priority>
              <lastmod>$(date -u +%Y-%m-%dT%H:%M:%S+00:00)</lastmod>
            </url>
          </urlset>
          EOF
          
      - name: Generate robots.txt
        run: |
          cat > site/robots.txt << 'EOF'
          User-agent: *
          Allow: /
          
          Sitemap: https://ai-news-daily.github.io/sitemap.xml
          EOF
          
      - name: Optimize and validate
        run: |
          # Validate JSON files
          echo "üîç Validating data files..."
          node -e "
            const fs = require('fs');
            try {
              JSON.parse(fs.readFileSync('data/latest-processed.json'));
              console.log('‚úÖ Data files valid');
            } catch(e) {
              console.error('‚ùå Invalid JSON:', e.message);
              process.exit(1);
            }
          "
          
          # Check if we have articles
          ARTICLE_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/latest-processed.json')).articles.length)")
          echo "üìä Found $ARTICLE_COUNT articles"
          
          if [ "$ARTICLE_COUNT" -lt 10 ]; then
            echo "‚ö†Ô∏è Warning: Low article count ($ARTICLE_COUNT)"
          fi
          
      - name: Commit and push changes
        run: |
          git config --local user.email "ai-news-bot@users.noreply.github.com"
          git config --local user.name "AI News Bot"
          
          # Add all changes including deletions
          git add data/ site/
          git add -u  # Stage deletions
          
          # Create commit message with stats
          ARTICLE_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/latest-processed.json')).articles.length)")
          TIMESTAMP=$(date -u '+%Y-%m-%d %H:%M UTC')
          
          # Check what's being committed
          git status --short
          
          git commit -m "ü§ñ Update AI news - $ARTICLE_COUNT articles - $TIMESTAMP

          üìä Latest: $ARTICLE_COUNT articles processed
          üóÇÔ∏è Historical: 15-day rolling archive maintained
          üöÄ Deployed: $(date -u '+%Y-%m-%d %H:%M UTC')" || {
            echo "No changes to commit"
            exit 0
          }
          
          git push
          
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./site
          
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
          
      - name: Report status
        run: |
          echo "‚úÖ Workflow completed successfully!"
          echo "üìä Statistics:"
          node -e "
            const data = JSON.parse(require('fs').readFileSync('data/latest-processed.json'));
            console.log(\`Articles: \${data.articles.length}\`);
            const categoryStats = {};
            data.articles.forEach(article => {
              const category = article.category || article.source_category || 'uncategorized';
              categoryStats[category] = (categoryStats[category] || 0) + 1;
            });
            console.log(\`Categories: \${Object.keys(categoryStats).length}\`);
            console.log(\`AI Processing: \${data.processingMethod || 'completed'}\`);
          "
          
      - name: Cleanup old data files (15-day rolling archive)
        run: |
          # Keep only last 15 days of date-specific data files
          echo "üßπ Cleaning up data files older than 15 days..."
          
          # Find and list files to be deleted (for logging)
          find data/ -name "20*-*-processed.json" -type f -mtime +15 | while read file; do
            echo "Deleting old file: $file"
          done
          
          # Delete old date-specific files (but keep latest-*.json files)
          find data/ -name "20*-*-processed.json" -type f -mtime +15 -delete || true
          
          # Also clean up corresponding site data files
          find site/data/ -name "20*-*.json" -type f -mtime +15 -delete 2>/dev/null || true
          
          echo "‚úÖ Rolling cleanup completed (15-day archive maintained)"
          
  # Health check job
  health-check:
    needs: crawl-and-build
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Check deployment health
        run: |
          sleep 60  # Wait for deployment
          
          # Check if site is accessible
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" "https://ai-news-daily.github.io/" || echo "000")
          
          if [ "$STATUS" = "200" ]; then
            echo "‚úÖ Site is healthy (HTTP $STATUS)"
          else
            echo "‚ùå Site health check failed (HTTP $STATUS)"
            exit 1
          fi

          
# Environment variables for the workflow
env:
  NODE_OPTIONS: '--max-old-space-size=4096'  # Increase memory for AI models
  ORT_LOG_LEVEL: '3'  # Suppress ONNX runtime warnings
  ONNX_DISABLE_WARNINGS: '1'
  ONNXRUNTIME_LOG_LEVEL: '3' 