{
  "crawledAt": "2026-01-03T02:03:12.023Z",
  "totalSources": 70,
  "totalArticles": 21,
  "aiFilterUsed": true,
  "articles": [
    {
      "title": "Has anyone used AI to find coupon codes?",
      "url": "https://www.reddit.com/r/artificial/comments/1q2hlov/has_anyone_used_ai_to_find_coupon_codes/",
      "source": "r/artificial",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-03T01:53:51.000Z",
      "metaDescription": "I was buying some furniture earlier today and when I was at checkout I saw there was a section for a coupon code. I checked all the normal sources for coupons like RetailMeNot and didn't find anything",
      "category": "ai-agents",
      "difficulty": 4,
      "confidence": 0.2933802595148419,
      "crawledAt": "2026-01-03T02:02:01.724Z",
      "id": "tbpkck",
      "entities": [
        {
          "entity": "B-MISC",
          "score": 0.984325647354126,
          "index": 4,
          "word": "AI",
          "start": null,
          "end": null
        }
      ],
      "summary": " I was buying furniture when I saw a section for a coupon code. I didn't find anything. I checked all the normal sources for coupons.",
      "language": "en",
      "processed_at": "2026-01-03T02:03:23.382Z"
    },
    {
      "title": "I guess Macys is using AI now ðŸ˜­ðŸ’€",
      "url": "https://www.reddit.com/r/artificial/comments/1q2h6sb/i_guess_macys_is_using_ai_now/",
      "source": "r/artificial",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-03T01:35:24.000Z",
      "metaDescription": "submitted by    /u/AdMedium5702  \n [link]   [comments]",
      "category": "creative-ai",
      "difficulty": 5,
      "confidence": 0.5735092814702955,
      "crawledAt": "2026-01-03T02:02:00.983Z",
      "id": "ja4v1x",
      "entities": [
        {
          "entity": "B-ORG",
          "score": 0.9884716272354126,
          "index": 3,
          "word": "Macy",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.8465592265129089,
          "index": 4,
          "word": "##s",
          "start": null,
          "end": null
        },
        {
          "entity": "B-MISC",
          "score": 0.9338321685791016,
          "index": 7,
          "word": "AI",
          "start": null,
          "end": null
        }
      ],
      "summary": "Community discussion about AI developments and related topics.",
      "language": "en",
      "processed_at": "2026-01-03T02:03:23.469Z"
    },
    {
      "title": "OpenAI 2026 Bust Scenario",
      "url": "https://www.reddit.com/r/singularity/comments/1q2gu8h/openai_2026_bust_scenario/",
      "source": "r/singularity",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-03T01:20:25.000Z",
      "metaDescription": "ChatGPT DAUs are going sideways, and subs revenue growth is decelerating. Current trajectory is to reach < 50% of their 2026 revenue plan.\n OpenAI may become the poster child of the AI infra bubble. M",
      "category": "creative-ai",
      "difficulty": 5,
      "confidence": 0.34608787688648984,
      "crawledAt": "2026-01-03T02:02:12.238Z",
      "id": "yczo71",
      "entities": [
        {
          "entity": "B-ORG",
          "score": 0.9455949068069458,
          "index": 1,
          "word": "Open",
          "start": null,
          "end": null
        },
        {
          "entity": "I-MISC",
          "score": 0.9316034317016602,
          "index": 7,
          "word": "##t",
          "start": null,
          "end": null
        },
        {
          "entity": "I-MISC",
          "score": 0.9409538507461548,
          "index": 8,
          "word": "Sc",
          "start": null,
          "end": null
        }
      ],
      "summary": " The current trajectory is to reach 50% of their 2026 revenue plan. OpenAI may become the poster child of the AI infra bubble.",
      "language": "en",
      "processed_at": "2026-01-03T02:03:33.009Z"
    },
    {
      "title": "Help with Z-Image Turbo LoRA training.",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q2gr54/help_with_zimage_turbo_lora_training/",
      "source": "r/StableDiffusion",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-03T01:16:37.000Z",
      "metaDescription": "Today, ten LoRAs were successfully trained; however, half of them exhibited glitchy backgrounds, featuring distorted trees, unnatural rock formations, and other aberrations. Guidance is sought on effe",
      "category": "creative-ai",
      "difficulty": 5,
      "confidence": 0.30010384375901084,
      "crawledAt": "2026-01-03T02:02:31.354Z",
      "id": "fozrel",
      "entities": [
        {
          "entity": "I-ORG",
          "score": 0.9608932137489319,
          "index": 5,
          "word": "Image",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.8643090128898621,
          "index": 6,
          "word": "Turbo",
          "start": null,
          "end": null
        }
      ],
      "summary": " Today, ten LoRAs were successfully trained. Half of them exhibited glitchy backgrounds, including distorted trees, unnatural rock formations, and other aberrations.",
      "language": "en",
      "processed_at": "2026-01-03T02:03:54.920Z"
    },
    {
      "title": "Nvidia just admitted the general-purpose GPU era is ending",
      "url": "https://venturebeat.com/infrastructure/inference-is-splitting-in-two-nvidias-usd20b-groq-bet-explains-its-next-act",
      "source": "VentureBeat",
      "source_domain": "venturebeat.com",
      "source_category": "news",
      "source_priority": "medium",
      "pubDate": "2026-01-03T01:00:00.000Z",
      "metaDescription": "Nvidiaâ€™s $20 billion strategic licensing deal with Groq represents one of the first clear moves in a four-front fight over the future AI stack. 2026 is when that fight becomes obvious to enterprise bu",
      "category": "creative-ai",
      "difficulty": 6,
      "confidence": 0.49079426737496673,
      "crawledAt": "2026-01-03T02:01:23.792Z",
      "id": "sjv83l",
      "entities": [
        {
          "entity": "B-ORG",
          "score": 0.9984169006347656,
          "index": 1,
          "word": "N",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.9885543584823608,
          "index": 2,
          "word": "##vid",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.9725658893585205,
          "index": 3,
          "word": "##ia",
          "start": null,
          "end": null
        },
        {
          "entity": "B-MISC",
          "score": 0.997402548789978,
          "index": 10,
          "word": "GP",
          "start": null,
          "end": null
        },
        {
          "entity": "I-MISC",
          "score": 0.9923564195632935,
          "index": 11,
          "word": "##U",
          "start": null,
          "end": null
        }
      ],
      "summary": " Nvidia's $20 billion licensing deal with Groq represents one of the first clear moves in the future. 2026 is when that fight becomes obvious.",
      "language": "en",
      "processed_at": "2026-01-03T02:04:06.387Z"
    },
    {
      "title": "Spectra-Etch",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q2g5jl/spectraetch/",
      "source": "r/StableDiffusion",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-03T00:50:52.000Z",
      "metaDescription": "Introducing Spectra-Etch LoRA for Z-Image Turbo\n Spectra-Etch is not just another LoRA.\n It deliberately pushes a modern Psychedelic Linocut aesthetic\n deep blacks, sharp neon contrasts, and rich wood",
      "category": "creative-ai",
      "difficulty": 3,
      "confidence": 0.9298506209559043,
      "crawledAt": "2026-01-03T02:02:32.083Z",
      "id": "7ccbza",
      "entities": [],
      "summary": " Spectra-Etch is a new look at the world's most popular music video series. Spectra is a collaboration between the band and the music group, which is based in New York.",
      "language": "en",
      "processed_at": "2026-01-03T02:04:20.942Z"
    },
    {
      "title": "AXRP Episode 47 - David Rein on METR Time Horizons",
      "url": "https://www.alignmentforum.org/posts/GHKYwjYtwzhukpBSb/axrp-episode-47-david-rein-on-metr-time-horizons",
      "source": "Alignment Forum",
      "source_domain": "alignmentforum.org",
      "source_category": "safety",
      "source_priority": "high",
      "pubDate": "2026-01-03T00:10:53.000Z",
      "metaDescription": "Published on January 3, 2026 12:10 AM GMT\n\nYouTube link\nWhen METR says something like â€œClaude Opus 4.5 has a 50% time horizon of 4 hours and 50 minutesâ€, what does that mean? In this episode David Rei",
      "category": "creative-ai",
      "difficulty": 6,
      "confidence": 0.7511672065285138,
      "crawledAt": "2026-01-03T02:01:33.260Z",
      "id": "9g5mum",
      "entities": [
        {
          "entity": "B-PER",
          "score": 0.9993171095848083,
          "index": 7,
          "word": "David",
          "start": null,
          "end": null
        },
        {
          "entity": "I-PER",
          "score": 0.9989757537841797,
          "index": 8,
          "word": "Re",
          "start": null,
          "end": null
        },
        {
          "entity": "I-PER",
          "score": 0.954221248626709,
          "index": 9,
          "word": "##in",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.8650397062301636,
          "index": 13,
          "word": "Time",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.9431854486465454,
          "index": 14,
          "word": "Horizon",
          "start": null,
          "end": null
        }
      ],
      "summary": " METR: \"Claude Opus 4.5 has a 50% time horizon of 4 hours and 50 minutes\"",
      "language": "en",
      "processed_at": "2026-01-03T02:04:54.957Z"
    },
    {
      "title": "Qwen3-4B-Thinking-2507 Usage inside Comfyui",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q2ekme/qwen34bthinking2507_usage_inside_comfyui/",
      "source": "r/StableDiffusion",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T23:43:30.000Z",
      "metaDescription": "So following my previous post about using Qwen3-4B-Thinking-2507 as a text encoder in replacement of Qwen3_4b for Z-image has been giving me better results due to the reasoning feature of this clip, i",
      "category": "creative-ai",
      "difficulty": 4,
      "confidence": 0.2610490391850044,
      "crawledAt": "2026-01-03T02:02:31.557Z",
      "id": "tsm22v",
      "entities": [
        {
          "entity": "B-LOC",
          "score": 0.9062157273292542,
          "index": 15,
          "word": "Co",
          "start": null,
          "end": null
        }
      ],
      "summary": " Qwen3-4B-Thinking-2507 as a text encoder has been giving me better results due to the reasoning feature of this clip.",
      "language": "en",
      "processed_at": "2026-01-03T02:05:35.021Z"
    },
    {
      "title": "I'm very confused: are people actually making money by selling agentic automations?",
      "url": "https://www.reddit.com/r/LangChain/comments/1q2eh8x/im_very_confused_are_people_actually_making_money/",
      "source": "r/LangChain",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-02T23:39:35.000Z",
      "metaDescription": "submitted by    /u/Ok-Introduction354  \n [link]   [comments]",
      "category": "ai-agents",
      "difficulty": 3,
      "confidence": 0.6042913545764355,
      "crawledAt": "2026-01-03T02:02:32.816Z",
      "id": "z3vvq8",
      "entities": [],
      "summary": "Community discussion about AI developments and related topics.",
      "language": "en",
      "processed_at": "2026-01-03T02:05:35.154Z"
    },
    {
      "title": "Pimp your ComfyUI",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q2dugp/pimp_your_comfyui/",
      "source": "r/StableDiffusion",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T23:13:37.000Z",
      "metaDescription": "submitted by    /u/neofuturist  \n [link]   [comments]",
      "category": "creative-ai",
      "difficulty": 3,
      "confidence": 0.9404787014131393,
      "crawledAt": "2026-01-03T02:02:29.291Z",
      "id": "msd3rh",
      "entities": [],
      "summary": "Community discussion about AI developments and related topics.",
      "language": "en",
      "processed_at": "2026-01-03T02:06:04.609Z"
    },
    {
      "title": "The AI Model That Learns While It Reads",
      "url": "https://www.reddit.com/r/singularity/comments/1q2cspg/the_ai_model_that_learns_while_it_reads/",
      "source": "r/singularity",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T22:31:12.000Z",
      "metaDescription": "A team from Stanford, NVIDIA, and UC Berkeley just reframed long-context modeling as a continual learning problem. Instead of storing every token explicitly, their model â€” TTT-E2E â€” keeps training whi",
      "category": "ai-agents",
      "difficulty": 5,
      "confidence": 0.3991372030946685,
      "crawledAt": "2026-01-03T02:02:12.407Z",
      "id": "lko7t8",
      "entities": [
        {
          "entity": "B-MISC",
          "score": 0.9969930648803711,
          "index": 2,
          "word": "AI",
          "start": null,
          "end": null
        },
        {
          "entity": "I-MISC",
          "score": 0.9668641686439514,
          "index": 3,
          "word": "Model",
          "start": null,
          "end": null
        }
      ],
      "summary": " Stanford, NVIDIA, and UC Berkeley reframed long-context modeling as a continual learning problem. Instead of storing every token explicitly, their model â€” TTT-E2E â€” keeps training whii.",
      "language": "en",
      "processed_at": "2026-01-03T02:06:53.820Z"
    },
    {
      "title": "Manus identified a bunch of drugs to activate an immune cell type. It's unbelievable what you can discover with AI agents that work for hours!",
      "url": "https://www.reddit.com/r/singularity/comments/1q2cgpw/manus_identified_a_bunch_of_drugs_to_activate_an/",
      "source": "r/singularity",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T22:18:06.000Z",
      "metaDescription": "submitted by    /u/Charuru  \n [link]   [comments]",
      "category": "ai-agents",
      "difficulty": 4,
      "confidence": 0.970467989637363,
      "crawledAt": "2026-01-03T02:02:11.041Z",
      "id": "i86qlk",
      "entities": [
        {
          "entity": "B-MISC",
          "score": 0.9128177165985107,
          "index": 25,
          "word": "AI",
          "start": null,
          "end": null
        }
      ],
      "summary": "Community discussion about AI developments and related topics.",
      "language": "en",
      "processed_at": "2026-01-03T02:06:53.996Z"
    },
    {
      "title": "DGX Spark Rack Setup and Cooling Solution",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2c520/dgx_spark_rack_setup_and_cooling_solution/",
      "source": "r/LocalLLaMA",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-02T22:05:10.000Z",
      "metaDescription": "If you own a DGX Spark you know that it can get pretty toasty during training runs. I built a DeskPI Rack and hooked up an automated temperature controller that controls the fan speed based on the cas",
      "category": "infrastructure",
      "difficulty": 3,
      "confidence": 0.44033244852166376,
      "crawledAt": "2026-01-03T02:02:09.667Z",
      "id": "8wjad",
      "entities": [],
      "summary": " The DGX Spark is an automated temperature controller that controls the fan speed of a fan. It can be used to control the fan fan speed based on the temperature of the fan.",
      "language": "en",
      "processed_at": "2026-01-03T02:07:17.307Z"
    },
    {
      "title": "Memory in Agentic AI",
      "url": "https://pub.towardsai.net/memory-in-agentic-ai-47f0bf0f0b6c?source=rss----98111c9905da---4",
      "source": "Towards AI",
      "source_domain": "pub.towardsai.net",
      "source_category": "medium",
      "source_priority": "high",
      "pubDate": "2026-01-02T22:02:26.000Z",
      "metaDescription": "Why Memory Matters for Autonomous Agents\nContinue reading on Towards AI Â»",
      "category": "ai-agents",
      "difficulty": 3,
      "confidence": 0.6226019389113818,
      "crawledAt": "2026-01-03T02:01:46.603Z",
      "id": "vby8i1",
      "entities": [],
      "summary": " Why Memory Matters for Autonomous Agents. Why Memory matters for Aut autonomous Agents? Read more on Towards AI. Read more from Towards AI.",
      "language": "en",
      "processed_at": "2026-01-03T02:07:26.751Z"
    },
    {
      "title": "Qwen Image 2512: Attention Mechanisms Performance",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q2bpvd/qwen_image_2512_attention_mechanisms_performance/",
      "source": "r/StableDiffusion",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T21:48:32.000Z",
      "metaDescription": "submitted by    /u/Ok-Significance-90  \n [link]   [comments]",
      "category": "creative-ai",
      "difficulty": 4,
      "confidence": 0.47705074163565864,
      "crawledAt": "2026-01-03T02:02:31.651Z",
      "id": "8grb3i",
      "entities": [
        {
          "entity": "B-ORG",
          "score": 0.99550861120224,
          "index": 1,
          "word": "Q",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.9707697033882141,
          "index": 2,
          "word": "##wen",
          "start": null,
          "end": null
        }
      ],
      "summary": "Discussion about AI-powered creative content generation and tools.",
      "language": "en",
      "processed_at": "2026-01-03T02:07:26.841Z"
    },
    {
      "title": "New tool: GridSplitter. Automatically extracts individual tiles from composite grid images",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q2bjby/new_tool_gridsplitter_automatically_extracts/",
      "source": "r/StableDiffusion",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T21:41:18.000Z",
      "metaDescription": "So I built GridSplitter to handle it automatically: \n - Extracts tiles from grid layouts instantly\n - Toggle between dark/light line detection\n - Adjust sensitivity for different image styles\n - Trim ",
      "category": "infrastructure",
      "difficulty": 3,
      "confidence": 0.3200871943243932,
      "crawledAt": "2026-01-03T02:02:30.911Z",
      "id": "xguumf",
      "entities": [],
      "summary": "GridSplitter to handle it automatically: Extracts tiles from grid layouts instantly instantly. Toggle between dark/light line detection and adjust sensitivity for different image styles.",
      "language": "en",
      "processed_at": "2026-01-03T02:07:37.627Z"
    },
    {
      "title": "Iâ€™ve been using and loving ChatGPT since the day it launched, but OpenAI broke it and itâ€™s time to move on.",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1q2berl/ive_been_using_and_loving_chatgpt_since_the_day/",
      "source": "r/ChatGPT",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T21:36:26.000Z",
      "metaDescription": "ChatGPT has been my conversational daily driver for years now, with Claude being my workhorse. But itâ€™s increasingly frustrating to interact with the AI now, despite their claims of benchmark superior",
      "category": "product-launch",
      "difficulty": 5,
      "confidence": 0.3958171793981566,
      "crawledAt": "2026-01-03T02:02:05.921Z",
      "id": "nmaaka",
      "entities": [
        {
          "entity": "B-ORG",
          "score": 0.9979289770126343,
          "index": 19,
          "word": "Open",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.995555579662323,
          "index": 20,
          "word": "##A",
          "start": null,
          "end": null
        },
        {
          "entity": "I-ORG",
          "score": 0.9902403354644775,
          "index": 21,
          "word": "##I",
          "start": null,
          "end": null
        }
      ],
      "summary": " ChatGPT has been my daily driver for years, with Claude being my workhorse. But it's increasingly frustrating to interact with the AI now, despite their claims of benchmark superior.",
      "language": "en",
      "processed_at": "2026-01-03T02:08:04.075Z"
    },
    {
      "title": "ðŸ³ Cook High Quality Custom GGUF Dynamic Quants â€” right from your web browser",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/",
      "source": "r/LocalLLaMA",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-02T20:59:07.000Z",
      "metaDescription": "I've just published a web front-end that wraps the GGUF Tool Suite's quant_assign.py so you can produce high-quality dynamic GGUF quants without touching the command line. Everything is integrated in ",
      "category": "creative-ai",
      "difficulty": 3,
      "confidence": 0.3552583099400717,
      "crawledAt": "2026-01-03T02:02:08.668Z",
      "id": "olecpe",
      "entities": [],
      "summary": "Everything is integrated in. Everything is integrated into the GGUF Tool Suite's quant_assign.py so you can produce high-quality dynamic quants without touching the command line.",
      "language": "en",
      "processed_at": "2026-01-03T02:08:38.665Z"
    },
    {
      "title": "welp",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1q29v1r/welp/",
      "source": "r/ChatGPT",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "medium",
      "pubDate": "2026-01-02T20:36:55.000Z",
      "metaDescription": "submitted by    /u/IntelligentDonut2244  \n [link]   [comments]",
      "category": "creative-ai",
      "difficulty": 3,
      "confidence": 0.39654472730705637,
      "crawledAt": "2026-01-03T02:02:03.896Z",
      "id": "9pjt4u",
      "entities": [],
      "summary": "Community discussion about AI developments and related topics.",
      "language": "en",
      "processed_at": "2026-01-03T02:08:46.379Z"
    },
    {
      "title": "How has AI changed your CS/IT studies?",
      "url": "https://www.reddit.com/r/OpenAI/comments/1q267sc/how_has_ai_changed_your_csit_studies/",
      "source": "r/OpenAI",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-02T18:21:21.000Z",
      "metaDescription": "I'm nearing the end of my Business Informatics degree and working part-time as a software developer. When I started my bachelor's in 2021, there was basically no AI to ask for help, especially for cod",
      "category": "creative-ai",
      "difficulty": 3,
      "confidence": 0.29994869962788256,
      "crawledAt": "2026-01-03T02:02:24.150Z",
      "id": "585ih0",
      "entities": [],
      "summary": " When I started my bachelor's in 2021, there was basically no AI to ask for help, especially for cod coders. I'm nearing the end of my Business Informatics degree and working part-time as a software developer.",
      "language": "en",
      "processed_at": "2026-01-03T02:09:12.231Z"
    },
    {
      "title": "I wrote a beginner-friendly explanation of how Large Language Models work",
      "url": "https://www.reddit.com/r/LangChain/comments/1q1y63d/i_wrote_a_beginnerfriendly_explanation_of_how/",
      "source": "r/LangChain",
      "source_domain": "reddit.com",
      "source_category": "reddit",
      "source_priority": "high",
      "pubDate": "2026-01-02T13:07:59.000Z",
      "metaDescription": "I recently published my first technical blog where I break down how Large Language Models work under the hood.\n The goal was to build a clear mental model of the full generation loop:\n  \ntokenization\n",
      "category": "creative-ai",
      "difficulty": 6,
      "confidence": 0.2880504364620194,
      "crawledAt": "2026-01-03T02:02:32.974Z",
      "id": "gbuuya",
      "entities": [
        {
          "entity": "B-MISC",
          "score": 0.9462851285934448,
          "index": 11,
          "word": "Large",
          "start": null,
          "end": null
        },
        {
          "entity": "I-MISC",
          "score": 0.9725273847579956,
          "index": 12,
          "word": "Language",
          "start": null,
          "end": null
        },
        {
          "entity": "I-MISC",
          "score": 0.9810482859611511,
          "index": 13,
          "word": "Models",
          "start": null,
          "end": null
        }
      ],
      "summary": " The goal was to build a clear mental model of the full generation loop. I recently published my first technical blog where I break down how Large Language Models work under the hood.",
      "language": "en",
      "processed_at": "2026-01-03T02:09:21.561Z"
    }
  ],
  "processedAt": "2026-01-03T02:09:21.568Z",
  "categories": [
    "ai-agents",
    "creative-ai",
    "infrastructure",
    "product-launch"
  ],
  "processingMethod": "ai-powered",
  "newArticlesProcessed": 21,
  "dailyArticlesCount": 21,
  "filterDate": "2026-01-03"
}